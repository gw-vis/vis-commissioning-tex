\subsection{Frequency series fitting using mathematical optimization}
In this section, we will be discussing how to model frequency series using optimization methods.
In particular, we will discuss the fitting of noise spectum measurments in Sec.~\ref{sec:noise_modeling_baseline}, as a follow up of Sec.~\ref{sec:sensor_noise_measurement}.
And, the fitting of transfer function measurements is discussed in Sec.~\ref{sec:transfer_function_modeling}.
Some general tips on fitting frequency series using optimization is given in Sec.~\ref{sec:optimization_tips}.
In Sec.~\ref{sec:curve_fitting_examples}, we will exemplify the methods discussed.
We will be using mathematical optimization notations as given in appendix \ref{appendix:optimization}.
A good \verb|SciPy| reference on mathematical optimization can be found in \cite{scipy_mathematical_optimization}.
In this section, we will refer the word ``optimization'' to ``mathematical optimization'', which is to minimize an objective function/cost function.
\subsubsection{Noise spectrum modeling \label{sec:noise_modeling_baseline}}
In this section, we will discuss how to obtain a noise model from some sensor readout.
This can be useful if we need to have a analytic model or if we want to estimate the sensor noise from a signal sensor, whose readout is not purely noise-dominated.

If conditions in the Sec.~\ref{sec:displacement_sensors_baseline} and Sec.~\ref{sec:inertial_sensors_baseline}, such as fixing the suspensions or measuring the readout before installation, are not available, we cannot use the aforementioned noise measurement methods.
But, we can still estimate the sensor noise via a curve fitting procedure, if we have a noise model and if the signal sensor readout is not dominated at all frequencies, i.e. signal only dominates spectral partially.
An example of this situation would be a sensor that measures a damped sinusoidal, i.e.
\begin{equation}
	y(t) = x(t) + n(t) = C\Re{\left(e^{\sigma+i\omega_n t}\right)} + n(t)\,,
	\label{eqn:sensor_readout_damp_sinusoidal}
\end{equation}
where $x(t) = C\Re{\left(e^{\sigma+i\omega_n t}\right)}$ is the signal, $C$ is a real number, $\sigma$ is a negative real number, $\omega_n$ is the oscillation frequency, and $n(t)$ is the sensor noise.
In this case, the frequency spectrum of signal $x(t)$ is localized around $f=2\pi\omega_n$, while the frequency content of the sensor noise $n(t)$ spreads all frequencies, which is very typical for motion sensors in a suspension.

Say, we have measurements
\begin{equation}
	\hat{Y}(f) = \left[\hat{X}(f)^2 + \hat{N}(f)^2\right]^{\frac{1}{2}}\,,
	\label{eqn:sensor_measurement}
\end{equation}
where $\hat{Y}(f)$ is the ASD of the sensor readout $y(t)$, $\hat{X}(f)$ is the ASD of some signal $x(t)$, and $\hat{N}(f)$ is the ASD of the sensor noise $n(t)$ that we want to measure/model.
If we have a noise amplitude spectral model $\hat{N}_\mathrm{model}(f;\theta_{\hat{N}_\mathrm{model}})$, where $\theta_{\hat{N}_\mathrm{model}}$ is a list of parameters that defines the noise $\hat{N}_\mathrm{model}$, then we can do a curve fit to obtain the noise model.
A example choice of the noise model would be a quadrature sum
\begin{equation}
	\hat{N}_\mathrm{model}(f;\theta_{\hat{N}_\mathrm{model}})=\left[\left(\frac{N_a}{f^a}\right)^2 + \left(\frac{N_b}{f^b}\right)^2\right]^{\frac{1}{2}}\,,
	\label{eqn:noise_model}
\end{equation}
where $\theta_{\hat{N}_\mathrm{model}}=\{N_a,N_b,a,b\}$ are the model parameters.

The fit can be as simple as a least squares fit, i.e. by minimizing the sum of error squares cost function
\begin{equation}
	J_\mathrm{lsq}\mleft(\theta;A(m), B(m,\theta)\mright) = \sum_m^M\left[A(m)-B(m,\theta)\right]^2 w(m)^2\,,
	\label{eqn:least_squares_cost_function}
\end{equation}
where $\theta$ is some model parameters of an arbitrary analytical function $B(m,\theta)$, $A(m)$ is the measurement data, $m$ is a common parametrization of the series $A$ and $B$, and $w(m)$ is a user-designed weighting of each data point (weighting function).
Typical choices for $m$ could be time $t$, frequency $f$, or simply data index.
Minimization of Eqn.~\eqref{eqn:least_squares_cost_function} will give you the optimal parameters $\theta^*$ such that the best fit of data $A$ is the model $B(\theta^*)$.

We can use the least squares fit directly but this is not the best for fitting measurement data that is typically viewed in log or log-log plots, where the value of data points varies drastically in orders of magnitude.
Instead, we use $J_\mathrm{lsq}\mleft(\theta;\log{A(m)}, \log{B(m,\theta)}\mright)$.

As for the weighting function $w(f)$ (now as a function of frequency), an easy choice for our purpose would be
\begin{equation}
	w(f)=
	\begin{cases}
		0 &\text{if } f_\mathrm{lower}<f<f_\mathrm{upper}\,,\\
		1 &\text{otherwise}\,,
	\end{cases}
	\label{eqn:weighting_function_frequency_bound}
\end{equation}
where $\left(f_\mathrm{lower},f_\mathrm{upper}\right)$ is a frequency bound that encloses frequency regions that has high SNR, indicated by the peak feature.
The choices of the design parameters $f_\mathrm{lower}$ and $f_\mathrm{upper}$ can't be easily told here as it requires users to look at the visualized data and make educated guesses.
Instead, we will demonstrate the choice of this bound in an example shown in Sec.~\ref{sec:curve_fitting_examples}.

To sum up, we have sensor data $\hat{Y}(f)$, which is an ASD (works for PSD as well.) as shown in Eqn.~\eqref{eqn:sensor_measurement}, and we would like to model the sensor noise $\hat{N}(f)$ with a model $\hat{N}(f;\theta_{\hat{N}_\mathrm{model}})$, where $\theta_{\hat{N}_\mathrm{model}}$ is a list of parameters of the sensor noise model.
We do so by minimizing the cost function
\begin{equation}
	\boxed{
		J_\mathrm{noise}\mleft(\theta_{\hat{N}_\mathrm{model}}\mright)=\sum_m^M\left[\log\hat{Y}(f_m)-\log\hat{N}_\mathrm{model}(f_m, \theta_{\hat{N}_\mathrm{model}})\right]^2 w(f_m)^2
	}\,,
	\label{eqn:noise_cost_function}
\end{equation}
where the weighting function $w(f)$ is given in Eqn.~\eqref{eqn:weighting_function_frequency_bound} and $f_m$ are the frequencies where the measured PSD and the model are evaluated.
The reason for using the log error $\log\hat{Y}(f) - \log\hat{N}_\mathrm{model}(f;\theta_{\hat{N}_\mathrm{model}})$ instead of the typical error is given in Sec.~\ref{sec:optimization_tips}, and will be obvious in the example section \ref{sec:curve_fitting_examples}.
Minimization gives best-fit parameters
\begin{equation}
	\theta_{\hat{N}_\mathrm{model}}^*=\arg\min_{\theta_{\hat{N}_\mathrm{model}}} \hat{J}_\mathrm{noise}\mleft(\theta_{\hat{N}_\mathrm{model}}\mright)\,,
\end{equation} such that the noise model $\hat{N}_\mathrm{model}\mleft(f;\theta_{\hat{N}_\mathrm{model}}^*\mright)$ becomes a best fit of the noise spectral density $\hat{N}(f)$.
The minimization can be done using methods that will be discussed in Sec.~\ref{sec:optimization_tips}.

\subsubsection{Some tips on using optimization \label{sec:optimization_tips}}
Minimization with measurement data can be done iteratively using optimization algorithms like gradient descent \cite{enwiki:1019572955} (local minimization) or stimulated annealing \cite{enwiki:1017509035} (global minimization).
We will not dive into the topic of mathematical optimization since it's a topic on it's own and is out of the scope of this document.
Instead, we will simply be using packages that are readily available.
A very useful Python submodule is \verb|scipy.optimize| \cite{scipy_optimize}.
It provides functions for both local minimization (\verb|scipy.optimize.minimize()|) and global minimization.
It comes with many popular algorithms such as the Nelder-Mead algorithm (\verb|scipy.optimize.minimize(..., method="Nelder-Mead")|) \cite{wiki:nelder_mead}, which is known to be good for optimization with noisy experiment data, or differential evolution \linebreak(\verb|scipy.optimize.differential_evolution()|) \cite{wiki:differential_evolution}.
The choice of algorithm is very subtle as different algorithms practically perform equally well for our purpose.
If you really need to know which one to use, try consulting \cite{scipy_mathematical_optimization}.
However, we do distinguish the choice between a local minimization algorithm and a global one.
The former can only be used when with guessed initial parameters, while the latter one can only be used when the boundaries of the parameters is known.
Typically, we\footnote{I mean I (Terrence)} prefer global optimization approaches because typically we don't know the parameters.
While the optimization result can depend on the initial guesses, having a poor guess would yield suboptimal results.

An important note on numerical optimization is parameter scaling \cite{wiki:preconditioner}.
We won't go into details.
The idea is to scale the parameters in such that they are all in the same scale, hence the cost function is equally sensitive to all parameters.
For some parameters $\theta=\{\theta_1,\theta_2\}$ that has a large dynamic range, it might be helpful if we optimize $\tilde{\theta}=\{\log\theta_1, \log\theta_2\}$ instead.
An example where this kind of rescaling is necessary is transfer function fitting with polynomial models, which have coefficients the vary in orders of magnitude.
Another example would be $N_a$ and $N_b$ from Eqn.~\eqref{eqn:noise_model}, which are noise levels that can also vary drastically.

Another useful pre-processing procedure would be to resample the data using an log-spaced frequency axis, instead of using the linearly spaced one as obtained during Fourier transform.
With linear-space frequency points, the majority of the data clusters at higher frequencies as viewed from a log-frequency axis, which create bias towards high frequency data.
As a result, the best-fit model might fit measurement data at higher frequencies better than that at lower frequencies.
A simple resample of the data will redistribute the data uniformly across the log-frequency axis.

\subsubsection{Examples \label{sec:curve_fitting_examples}}
\paragraph{Noise spectrum modeling}
Here, we will demonstrate how to use optimization to minimize cost functions \eqref{eqn:least_squares_cost_function} and \eqref{eqn:noise_cost_function} to model a sensor noise ASD $\hat{N}(f)$.
The notebook is available \href{https://kontrol.readthedocs.io/en/latest/tutorials/noise_spectrum_modeling_with_optimization.html}{here}.
We will assume a sensor readout $y(t)$ that follows Eqn.~\eqref{eqn:sensor_readout_damp_sinusoidal}, with a noise $n(t)$ that follows Eqn.~\eqref{eqn:noise_model}.
We will add measurement noise uniformly drawn from $[-3, 3]\,\mathrm{dB}$ to the sensor readout $y(t)$.

Fig.~\ref{fig:tutorialsnoisespectrummodelingwithoptimization10} shows the time series of the signal $x(t)$ and the frequency series $\hat{Y}(f)$, $\hat{N}(f)$, $\hat{X}(f)$, and the weighting function $w(f)$.
Here, the signal is
\begin{equation}
	x(t) = A\Re{\left(e^{\sigma+i\omega_n t}\right)},
\end{equation}
and we set $A=1$, $\sigma=-0.001$, $\omega_n=2\pi$.
The noise spectral density is
\begin{equation}
	\hat{N}(f) = \left[\left(\frac{N_{3.5}}{f^{3.5}}\right)^2+\left(\frac{N_1}{f^1}\right)^2\right]^\frac{1}{2}\,,
\end{equation}
where $N_{3.5}=\left(5\times10^{-7}\right)^\frac{1}{2}$ and $N_1=\left(1\times10^{-7}\right)^\frac{1}{2}$.
These frequency dependencies ($-3.5$ and $-1$) are chosen to mimic the behavior of the geophone noise.
This kind of sensor noise profiles has large amplitude variations at different frequencies so it can be used as a benchmark problem for noise modeling.
So, the ASD of the sensor readout, in $\mathrm{dB}$ is
\begin{equation}
	20\log\hat{Y}(f) = 20\log\left[\hat{X}(f)^2 + \hat{N}(f)^2\right]^\frac{1}{2} + \mathcal{U}(-3, 3)\,,
\end{equation}
where $\mathcal{U}(a,b)$ is a random series drawn from a uniform distribution between $a$ and $b$.

In reality, the only information we have is the sensor readout $\hat{Y}(f)$, as given by the green curve shown in Fig.~\ref{fig:tutorialsnoisespectrummodelingwithoptimization10}.
From the plot, we see a peek feature around $0.7 - 2\,\mathrm{Hz}$, which indicates the presence of the signal $x(t)$.
This is something that we do not wish to model.
Hence, we set the weighting function
\begin{equation}
	w(f)=
	\begin{cases}
		0 & \text{if }0.7<f<2\,\mathrm{Hz}\,,\\
		1 & \text{otherwise}\,,
	\end{cases}
\end{equation}
so it filters out the part of the readout that we don't wish to model.
\begin{figure}[!h]
	\centering
	\includegraphics[width=1\linewidth]{figures/tutorials_noise_spectrum_modeling_with_optimization_1_0}
	\caption{(Left) Time series of the signal $x(t)$. (Right) Amplitude spectral densities of the readout $\hat{Y}(f)$, sensor noise $\hat{N}(f)$, signal $\hat{X}(f)$, and the designed weighting function $w(f)$.}
	\label{fig:tutorialsnoisespectrummodelingwithoptimization10}
\end{figure}

As for the model, we model ~\eqref{eqn:noise_model}.
We fit the measurement with 4 different configurations.
For the first two, we use least squares fit, which uses Eqn.~\eqref{eqn:least_squares_cost_function}, with model parameters $\theta_{\hat{N}_\mathrm{model}}=\{N_a, N_b, a, b\}$, and ``log parameters'' $\tilde{\theta}_{\hat{N}_\mathrm{model}}=\{\log N_a, \log N_b, a, b\}$, respectively.
For the other two, we use least squares fit with log error, which uses Eqn.~\eqref{eqn:noise_cost_function}, again with the two different sets of model parameters.
The motivation for using log parameters is discussed in Sec.~\ref{sec:optimization_tips}.


\begin{figure}[!h]
	\centering
	\includegraphics[width=1\linewidth]{figures/tutorials_noise_spectrum_modeling_with_optimization_8_0}
	\caption{(Left) Fitting results. (Right) Residuals, i.e. error squared.}
	\label{fig:tutorialsnoisespectrummodelingwithoptimization80}
\end{figure}

Fig.~\ref{fig:tutorialsnoisespectrummodelingwithoptimization80} shows the optimization results with the four configurations.
As can be seen on the left plot, least squares fit with log error yields noise models that fit the actual noise well across all frequencies.
But, the traditional least square fit fails at fitting the data at higher frequencies, where the amplitudes of the noise is orders of magnitude lower than that at lower frequencies.
Right plot in Fig.~\ref{fig:tutorialsnoisespectrummodelingwithoptimization80} shows the residuals, defined by the square error between the actual noise and the fit, of these four configuration.
As shown in the plot, modeling the noise with least square fit with log error and log parameters yields the best result in default settings.
This shows the importance in correct scaling of the model parameters.

For all these results, we used \verb|scipy.optimize.minimize()|, with default settings, to minimize the cost functions, and we start from an initial guess of $\{0, 0, 0, 0\}$ in all cases.
We emphasize that all these four configuration may perform equally well if we further tweak the optimization parameters in \verb|scipy.optimize.minimize()|, or using different optimization algorithms.
There are many optimization tweaks that can improve the fitting performance, but we won't discuss here.
The results shown in Fig.~\ref{fig:tutorialsnoisespectrummodelingwithoptimization80} only serves as a demonstration of how a careful design of the model and the cost functions can yield better results under the same settings.

\subsubsection{Transfer function modeling \label{sec:transfer_function_modeling}}
In this section, we will discuss how the frequency response of a system can be modeled by a transfer function.
We will first discuss a heuristic way, where a closed-form of the transfer function can be guessed from the Bode plots of the frequency response.
Then, we will discuss how we can turn the transfer function fitting into an optimization problem (much like Sec.~\ref{sec:noise_modeling_baseline}).
The results obtained from the heuristic approach gives a good initial guess for the fitting approach and is the recommended way.
There exists other exotic methods to get an initial guess but they are rather tedious considering the standard KAGRA workflow.

A generic (stable) transfer function model can be written as
\begin{equation}
	G(s) = k\frac{\prod_i^a (s-z_i)}{\prod_i^b (s-p_i)}\frac{\prod_i^c \left(s^2 + \frac{\omega_i}{q_i}s + \omega_j^2\right)}{\prod_i^d \left(s^2+\frac{\Omega_i}{Q_i}s + \Omega_i^2\right)}\,,
	\label{eqn:generic_tf}
\end{equation}
where $s$ is the Laplace variable, $k\in\mathbb{R}$, $z_i\in\mathbb{R}^-$ are the simple zeros, $p_i\in\mathbb{R}^-$ are simple poles, $\omega_i\in\mathbb{R}^+$ $\Omega_i\in\mathbb{R}^+$ are anti-resonance/resonance frequencies  (rad/s) of a pair of complex zeros and a pair of complex poles respectively, $q_i\in[0.5, \infty]$ and $Q_i\in[0.5, \infty]$ their quality factors, and $a$, $b$, $c$, and $d$ are the number of simple zeros, simple poles, pairs of complex zeros and pairs of complex poles respectively.
We can rewrite Eqn.~\eqref{eqn:generic_tf} into
\begin{equation}
	G(s) = k_\mathrm{DC}\frac{\prod_i^a (-s/z_i+1)}{\prod_i^b (-s/p_i+1)}\frac{\prod_i^c \left(s^2/\omega_i^2 + \frac{1}{\omega_i q_i}s + 1\right)}{\prod_i^d \left(s^2/\Omega_i^2+\frac{1}{\Omega_i Q_i}s + 1\right)}\,,
	\label{eqn:generic_tf_normalized}
\end{equation}
where $K_\mathrm{DC}$ is the magnitude of the transfer function at zero frequency, i.e. DC gain.
This form is particular useful for manual fitting of a transfer function, as we will show later.
Very often we plot the magnitude response of the transfer function in a log-scale plot.
In a magnitude plot (Bode magnitude plot), $s$ is evaluated along the imaginary axis $j\omega$, where $j$ is the imaginary number.
So, taking the logarithm and substituting $s=j\omega$ in Eqn.~\eqref{eqn:generic_tf_normalized} yields
\begin{equation}
	\begin{split}
	\log\left\lvert G(j\omega)\right\rvert = &\log K_\mathrm{DC} + \log \sum_i \left\lvert -j\omega/z_i+1 \right\rvert
	-\log \sum_i \left\lvert -j\omega/p_i+1\right\rvert\\
	&+\log \sum_i \left\lvert -\omega^2/\omega_i^2 + \frac{1}{\omega_i q_i} + 1\right\rvert
	-\log \sum_i \left\lvert -\omega^2/\Omega_i^2 + \frac{1}{\Omega_i Q_i} + 1\right\rvert\,.
	\end{split}
	\label{eqn:generic_tf_normalized_magnitude}
\end{equation}
Now, let's inspect Eqn.~\eqref{eqn:generic_tf_normalized_magnitude} and see how we can plot the transfer function qualitatively.
Consider the case at DC, i.e. $\omega=0$, the last 4 terms become $\log 1 = 0$ and the remaining term is $\log K_{\mathrm{DC}}$.
Therefore, the magnitude plot starts at $K_\mathrm{DC}$.
Let's assume that $z_i\neq p_i \neq \omega_i \neq \Omega_i$ and consider the case as $\omega$ becomes much greater than one of $\lvert z_i\rvert$, $\lvert p_i\rvert$, $\omega_i$, or $\Omega_i$.
As $\omega$ approaches much greater than $\lvert z_i\rvert$, $\lvert p_i\rvert$, $\omega_i$, or $\Omega_i$, the higher order terms become dominant.
Take the case of simple zeros for example, $\log \left\lvert -j\omega/z_i+1\right\rvert\approx \log \lvert -j\omega/z_i \rvert = \log \omega - \log z_i$.
This corresponds to $+1$ change in slope in the log-log plot of the magnitude response.
In other words, the frequency dependency of the magnitude response becomes $\omega^{n+1}$ as $\omega$ approaches a $\lvert z_i\rvert$, where $\omega^n$ is the frequency dependency of $\lvert G(j\omega) \rvert$ as $\omega\ll z_i$.
With the same reasoning, we can conclude that as $\omega$ approaches each simple pole $p_i$, the frequency dependency exponent of the $\lvert G(j\omega)\rvert$ decreases by 1 (because of the minus sign in Eqn.~\eqref{eqn:generic_tf_normalized_magnitude}), which correspond to change of -1 in slope in the log-log plot.
Similarly, as $\omega$ approaches resonance frequencies $\omega_i$/$\Omega_i$, the exponent increases/decreases by 2.
For complex pairs of zeros/poles, the magnitude response has an anti-resonance (a notch) at $\omega=\omega_i$ and has a resonance (a peak) at $\omega=\Omega_i$.
The depth/height of the notch/peak depends on the corresponding Q factors $q_i$ and $Q_i$.

For an example, consider the transfer function
\begin{equation}
	G(s) = 2 \frac{(s/1 + 1)}{(s/10 + 1)^2}\frac{(s^2/100^2 + \frac{1}{100\times 30}s + 1)}{(s^2/1000^2 + \frac{1}{1000\times 300}s + 1)}\,.
	\label{eqn:example_transfer_function}
\end{equation}
First of all, the DC gain is at $K_\mathrm{DC}=2$, so the magnitude is 2 at frequency lower than all the zeros/poles and resonances/anti-resonances.
The second lowest frequency feature is a simple zero at $\omega=1$.
From there, the slope changes by +1.
Then, there's a double simple pole at $\omega=10$.
The slope changes by -2 at $\omega=10$.
After that, there's an anti-resonance at $\omega=100$ with a Q factor of 30.
At last, there's a resonance at $\omega=1000$ with a Q factor of 300.
The slope changes by +2 (anti-resonance) and -2 (resonance).
All of these features are shown in Fig.~\ref{fig:exampletransferfunctionplot}.

\begin{figure}[!ht]
	\centering
	\includegraphics[width=1\linewidth]{figures/example_transfer_function_plot}
	\caption{Magnitude response of Eqn.~\eqref{eqn:example_transfer_function}}
	\label{fig:exampletransferfunctionplot}
\end{figure}

In summary, simple zeros/poles increase/decrease the slope of the magnitude response, while complex zeros/poles increase/decrease the slope of the magnitude response and making notches/peaks at the resonance frequency.

To manually fit the frequency response of a system, we need to identify the ``features" that are present in the frequency response \footnote{Wouldn't it be nice if we can train a neural network to identify them automatically :)?}.
These features are the aforementioned ones such as changes in slope, notches, and peaks in the magnitude response.
For example, without knowing Eqn.~\eqref{eqn:example_transfer_function}, looking at Fig.~\ref{fig:exampletransferfunctionplot}, it's obvious that there's an increase in slope at $\omega=0$, a (double) decrease in slope at $\omega=10$, a notch at $\omega=100$, and a peak at $\omega=1000$.
From here, we can immediately tell that there're one simple zero, two simple poles, one pair of complex zeros, and one pair of complex poles.
At the lowest frequency of the plot, the magnitude response is close to plateau at a value of 2, indicating that the DC gain is 2.
After that, the remaining parameters are the Q factors of the resonances and the anti-resonance.
There exists ways to obtain the Q factors from the peak and width of the resonances.
But, we better off manually tweaked the Q factors to match plot as this is something very easy to do.



Suspensions are multiple pendulums, which can be modeled by damped oscillatory systems.
Typically, their transfer functions can be modeled by a superposition of complex poles pairs. 
